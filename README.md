# E-commerce Data Pipeline ðŸ§±

End-to-end **batch data pipeline** project built to showcase practical data engineering skills:

- Ingests raw e-commerce orders from CSV
- Cleans, transforms and validates data with pandas
- Loads a **star schema** (fact + dimensions) into SQLite (easily swappable to Postgres)
- Runs analytics queries and exports **reports** for business users
- Includes **data quality checks**, **logging**, **CLI with dry-run**, **tests**, and **CI with GitHub Actions**

> Built as a portfolio project for data engineer roles â€“ designed to be small but realistic and easy to explain in interviews.

---

## 1. Project Overview

**Use case**

Simulate a real-world e-commerce analytics setup:

- Raw orders land in a CSV file
- We need clean, validated data in a warehouse
- Analysts want:
  - Top products by revenue
  - Revenue by month
  - Revenue by country / customer

**What this repo demonstrates**

- End-to-end pipeline: **ingest â†’ transform â†’ validate â†’ load â†’ report**
- Proper **project structure** (`src/`, `tests/`, `sql/`, `docs/`)
- **Dimensional modeling**: fact table + dimension tables
- **Data quality layer** that can fail the pipeline on bad data
- **Observability**:
  - structured logging to file + console
  - JSON run summary
- **Analytics layer** on top of the warehouse
- **Automated tests** and **GitHub Actions CI**

---

## 2. Tech Stack

- **Language**: Python 3.x  
- **Data processing**: pandas
- **Storage / warehouse**: SQLite (via SQLAlchemy, easily switchable to other DBs)
- **Orchestration**: simple Python CLI (could be wrapped in Airflow/Prefect later)
- **Config**: `python-dotenv` + centralized `config.py`
- **Testing**: pytest
- **CI**: GitHub Actions (`.github/workflows/ci.yml`)

---

## 3. Data & Model

### 3.1 Raw data

File: `data/raw/orders_raw.csv`

Columns:

- `order_id`
- `customer_id`, `customer_name`, `country`
- `product_id`, `product_name`, `category`
- `order_date`
- `quantity`
- `unit_price`

Current sample dataset (for demo):

- ~40+ orders
- Multiple customers across several countries (Germany, France, Italy, Spain, â€¦)
- Multiple products & categories (Electronics, Accessories, Stationery, Home)
- Date range across multiple months in 2024

### 3.2 Warehouse schema (star schema)

Data is loaded into a simple star schema inside `warehouse.db`:

**Dimension: `dim_customers`**

- `customer_id` (PK)
- `customer_name`
- `country`

**Dimension: `dim_products`**

- `product_id` (PK)
- `product_name`
- `category`

**Fact: `fact_orders`**

- `order_id` (PK)
- `customer_id` (FK â†’ `dim_customers.customer_id`)
- `product_id` (FK â†’ `dim_products.product_id`)
- `order_date`
- `quantity`
- `unit_price`
- `total_amount` (`quantity * unit_price`)

This model supports common analytics like:

- Revenue by product / category
- Revenue by month
- Revenue by country / customer

---

## 4. Project Structure

```bash
ecommerce-data-pipeline/
â”œâ”€ data/
â”‚  â”œâ”€ raw/              # Input CSV (orders_raw.csv)
â”‚  â””â”€ processed/        # Cleaned CSV (orders_clean.csv)
â”œâ”€ docs/
â”‚  â””â”€ design.md         # Detailed design document
â”œâ”€ logs/
â”‚  â”œâ”€ pipeline.log      # Structured logs
â”‚  â””â”€ run_summary.json  # JSON summary of last run
â”œâ”€ reports/
â”‚  â”œâ”€ top_products_by_revenue.csv
â”‚  â”œâ”€ revenue_by_month.csv
â”‚  â””â”€ revenue_by_country.csv
â”œâ”€ sql/
â”‚  â””â”€ analytics_queries.sql   # Reference SQL for analytics
â”œâ”€ src/
â”‚  â”œâ”€ analytics/
â”‚  â”‚  â””â”€ reports.py           # Reporting/analytics functions
â”‚  â”œâ”€ ingestion/
â”‚  â”‚  â””â”€ ingest_orders.py     # Read raw CSV
â”‚  â”œâ”€ transformations/
â”‚  â”‚  â”œâ”€ transform_orders.py  # Clean + feature engineering
â”‚  â”‚  â””â”€ data_quality.py      # Data validation rules
â”‚  â”œâ”€ warehouse/
â”‚  â”‚  â”œâ”€ db.py                # SQLAlchemy engine
â”‚  â”‚  â””â”€ load_to_db.py        # Create schema + load tables
â”‚  â”œâ”€ orchestration/
â”‚  â”‚  â””â”€ pipeline.py          # Orchestrates pipeline steps
â”‚  â”œâ”€ utils/
â”‚  â”‚  â””â”€ logging_utils.py     # Console + file logger
â”‚  â”œâ”€ config.py               # Paths, DB_URL, dirs
â”‚  â”œâ”€ main.py                 # CLI entrypoint for pipeline
â”‚  â””â”€ run_analytics.py        # CLI entrypoint for reports
â”œâ”€ tests/
â”‚  â”œâ”€ test_transformations.py
â”‚  â”œâ”€ test_data_quality.py
â”‚  â””â”€ test_analytics.py       # Analytics queries on in-memory DB
â”œâ”€ .github/workflows/ci.yml   # GitHub Actions: run tests
â”œâ”€ .gitignore
â”œâ”€ requirements.txt
â””â”€ README.md
